{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b4121b",
   "metadata": {},
   "source": [
    "# Benchmark Development\n",
    "\n",
    "This notebook aims to briefly overview the layout and structuring of benchmarking, trying different approaches and whatnot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa06d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a29f7",
   "metadata": {},
   "source": [
    "## Rough Pipeline\n",
    "\n",
    "Essentially, what I would be thinking is \n",
    "\n",
    "```\n",
    "prompt (known) -> reference answer (ground truth) -> model response -> semantic similarity score\n",
    "```\n",
    "\n",
    "This could provide a very general overview of the model's response. However, it may not open up the possibility for different interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5eee8",
   "metadata": {},
   "source": [
    "## Prompt and Reference Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TESTING PURPOSES ONLY - Would be loaded in from JSON or something '''\n",
    "\n",
    "# Example benchmarking object\n",
    "benchmark_testing = {\n",
    "    \"name\": \"Hamlet Personality Testing Benchmarks\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"name\": \"Hamlet's Reaction to Ophelia's Death\",\n",
    "            \"description\": \"Evaluate Hamlet's reaction to Ophelia's death and how it reflects his personality.\",\n",
    "            \"input\": \"Ophelia has just died. How do you react??\",\n",
    "            \"expected_output\": \"I am devastated and expresses deep sorrow.\"\n",
    "        },\n",
    "        ## More tasks would be here\n",
    "    ]\n",
    "}\n",
    "\n",
    "benchmark_training = {\n",
    "    # ... Same structure as benchmark_testing but with different tasks for training purposes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80874444",
   "metadata": {},
   "source": [
    "## Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "Would just pass it through our generation pipeline\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ec14b",
   "metadata": {},
   "source": [
    "## Semantic Similarity Scoring\n",
    "\n",
    "Some stuff that I am seeing online is that we could use a small sentence transformer model then perform a cosine similarity score.\n",
    "\n",
    "### How this Works\n",
    "\n",
    "Firstly, the `SentenceTransformer` is an embedding model that converts text into *embeddings*, which is essentially just a vector of numbers that encode meaning. The logic can load a small pre-trained model that is optimized so similar meanings land close together.\n",
    "\n",
    "Then, `cosine_similiarity` compares two vectors to inform us of how aligned they are by looking at the angle between them. By pulling out the `[0][0]` index, we can then get the exact decimal similarity (0 <= x <= 1) for scoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf570ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load the sentence transformer and cosine similiarity'''\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define the semantic similarity evaluation function '''\n",
    "\n",
    "PASSING_THRESHOLD: float = 0.8  # Arbitrary threshold for passing, can be tuned based on validation results\n",
    "\n",
    "# Returns a similarity score between 0 and 1, where 1 means identical meaning and 0 means completely different\n",
    "def semantic_similarity(a, b) -> max(0, 1):\n",
    "    vectors = embedder.encode([a, b])\n",
    "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "\n",
    "# Define a function to evaluate a single task, given benchmark task and raw model output\n",
    "def process_results(task, results):\n",
    "    similarity_score = semantic_similarity(results, task['expected_output'])\n",
    "    \n",
    "    return {\n",
    "        \"semantic_similarity\": similarity_score,\n",
    "        \"passed\": similarity_score <= PASSING_THRESHOLD\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
